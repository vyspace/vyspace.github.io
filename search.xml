<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Jupyter 部署]]></title>
    <url>%2Fai%2Fhadoop-jpt%2F</url>
    <content type="text"><![CDATA[Jupyter Notebook 是一个Python在线编辑器，在机器学习领域很流行，调试代码也很方便。当然你可以选择其他编辑器。因为后面的示例会有一部分使用Python完成，所以这篇我们来部署Jupyter。 Quick StartAnaconda安装Anaconda是Python的版本管理工具，登陆官方网站下载安装包，其安装文件分为Python3.x与Python2.7版本。 首先选择一个主要Python版本，这里选择2.7，所以我们下载Anaconda(py2)，下载可直接运行sh文件进行安装。安装完成后可运行如下指令查看conda的信息： 1conda info 切换Python3运行如下命令，安装Python3 1conda create -n py3 python=3 版本切换回到py2 1source deactivate py3 进入py3 1source activate py3 除了Anaconda，你也可以安装它的mini版，Miniconda。 Anaconda包内已经包含了Jupyter，在装完anaconda应该会自动安装了Jupyter，下面就可以直接启动Jupyter服务了。 Jupyter注：启动Jupyter服务建议不要使用root账户，原因后面会说到，所以我们切换到其他账户，这里我切换到test账户： 1su test 生成配置文件1jupyter notebook --generate-config 生成后的文件会在~/.jupyter中 自动生成密钥因为服务开放后，所有人都可以访问，所以需要配置密码（此步也是必须的）。 1jupyter notebook password 运行此命令后，可输入两次密码，完成后会在~/.jupyter/upyter_notebook_config.json文件中生成一串token。 配置HTTP服务打开~/.jupyter/jupyter_notebook_config.py文件，修改如下配置项： 1234567891011121314# 容许所有IP可访问c.NotebookApp.ip = &apos;*&apos;# 初始化notebook工作区根目录，我在test帐户下，新建jupyter_notebook文件夹作为根目录c.NotebookApp.notebook_dir = u&apos;/home/test/jupyter_notebook&apos;＃ 是否打开浏览器立即启动c.NotebookApp.open_browser = False＃ 之前生成的tokenc.NotebookApp.password ＝ u&apos;sha1:xxx&apos;＃ 端口配置c.NotebookApp.port = 8888 运行服务1jupyter notebook --config=~/.jupyter/jupyter_notebook_config.py 测试启动完成后，浏览器访问8888端口，就可以正常打开Jupyter了， 未完待续…]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 部署]]></title>
    <url>%2Fai%2Fhadoop-hbs%2F</url>
    <content type="text"><![CDATA[完成之前的章节，我们已经将Hadoop集群与Spark计算引擎成功部署在4个节点中了。你可以使用Java或者Scala语言（这里推荐Scala）进行开发，并可以用Spark正常进行数据挖掘了。这章我们讲HBase的部署，基本与数据存储有关。 Quick Start下载安装按照之前的规划表，我们会在node3中启动HBase的主进程，在node2中启动备用进程，所以在这篇文章我们选择在node3中进行配置。 登陆node3节点，并下载HBase安装包，版本1.3.1，下载完成后解压（文件目录还是统一放在/opt路径下）并进入该文件夹。 基础配置打开conf/hbase-site.xml文件，在configuration标签中添加如下配置项： 启动集群模式1234&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; HDFS中设置HBase主目录1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;&lt;/property&gt; ZooKeeper集群地址1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0的IP, node2的IP, node3的IP&lt;/value&gt;&lt;/property&gt; ZooKeeper快照存储位置注：此项与zoo.conf中dataDir路径相同。 1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/zookeeper-3.4.10/zk_data&lt;/value&gt;&lt;/property&gt; 配置元数据存储节点HBase中的数据分为元数据（文件索引）与文件本身数据，文件数据由DataNode负责存取，元数据则由HRegionServer负责。按照规划表，我们会把元数据分布在4台节点中，所以我们需要在所有节点中部署HRegionServer，配置方法如下： 在conf文件夹中打开regionservers文件（如果未找到，新建即可）。添加如下内容： 1234node0的IPnode1的IPnode2的IPnode3的IP 配置环境变量1234567891011# Java环境变量是必不可少的。export JAVA_HOME=/opt/jdk1.8.0_65# 因为HBase自身就带有一个ZooKeeper，非集群模式时，我们可以用它自己带的就好，集群模式下关闭它，防止启动多个ZooKeeperexport HBASE_MANAGES_ZK=false# 让HBase可以找到Hadoop的配置文件hdfs-site.xml，这里配置目录路径就好。export HBASE_CLASSPATH=/opt/hadoop-2.7.4/etc/hadoop# HBase工作目录路径（tmp文件夹是我自己创建的，你也可以指定到别的路径下）export HBASE_PID_DIR=/opt/hbase-1.3.1/tmp 分发安装包将配置好的HBase文件夹拷贝到所有节点中 123scp -r /opt/hbase-1.3.1 root@node0:/optscp -r /opt/hbase-1.3.1 root@node1:/optscp -r /opt/hbase-1.3.1 root@node2:/opt 启动服务在node3上执行如下命令： 1bin/start-hbase.sh 执行完成后使用jps命令进行查看，node3中会有HMaster和HRegionServer服务 HA为了达到高可用，我们需要启动一个备用进程，按照规划图，在node2中运行如下命令： 1bin/hbase-daemon.sh start master Web 访问在浏览其中输入地址可以访问HMaster 1node3的IP:16010 访问HRegionServer 1node3的IP:16030 测试SHELL在命令行中输入如下命令，进入hbase shell界面后，可执行一些基础操作。 1bin/hbase shell 注：HBase的命令与其它数据库（例如：MySql）不同，命令行结束后不能加分号，名称（表名）要加引号（双引号或单引号） 创建表1create &apos;表名&apos;, &apos;列族&apos; 查看所有表1list 查看表属性1describe &apos;表名&apos; 插入数据1put &apos;表名&apos;, &apos;rowkey&apos;, &apos;列族:列&apos;, &apos;值&apos; 查看表中所有数据1scan &apos;表名&apos; 以上是一些基础命令的测试，如果你对HBase的shell操作有更多的兴趣，请点击下方官方文档进行查阅。 官方文档如果需要了解更详细的内容，请访问官方文档 小结完成上述配置后，HBase可以正常访问了，基础的存储与计算都配置完成。下篇文件我们开始《Jupyter 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 部署]]></title>
    <url>%2Fai%2Fhadoop-spk%2F</url>
    <content type="text"><![CDATA[Spark是内存式计算引擎，为了让我们的计算速度更快，计算更多复杂的模型。这篇文章我们部署它，之后我们编写的代码都将跑在Spark中。 Quick Start下载安装登陆官方网站，下载版本为2.2.1，下载完成后解压（文件目录还是统一放在/opt路径下）并进入该文件夹，运行如下命令： 注：从2.0版本开始，缺省支持Scala2.11版本，如果你习惯使用其他版本的Scala，请查看官网 1cp conf/spark-env.sh.template conf/spark-env.sh 环境变量配置在conf/spark-env.sh文件中添加如下配置项： Java环境变量1export JAVA_HOME=$JAVA_HOME 如果未设置Java环境变量，请自行添加就好。 Client模式运行时，所需的参数环境变量123456789101112131415# yarn集群中，最多能够同时启动的Executors的实例个数。# yarn中实际能够启动的最大Executors的数量会小于等于该值。如果不能确定最大能够启动的Executors数量，建议将该值先设置的足够大。export SPARK_ EXECUTOR_INSTANCES=9# 该参数为设置每个Executor能够使用的CPU核数# yarn集群能够最多并行的task数量为SPARK_EXECUTOR_INSTANCES ＊ SPARK_EXECUTOR_CORESexport SPARK_EXECUTOR_CORES=2# 该参数设置的是每个Executor分配的内存的数量。# 需要注意的是，该内存数量是SPARK_EXECUTOR_CORES中设置的内核数共用的内存数量。export SPARK_EXECUTOR_MEMORY=8G#该参数设置的是DRIVER分配的内存的大小。#也就是执行start-thriftserver.sh机器上分配给thriftserver的内存大小。export SPARK_DRIVER_MEMORY=8G ZooKeeper环境变量1export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node0的IP:2181,node2的IP:2181,node3IP:2181" 其他环境变量12345export HADOOP_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoopexport YARN_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoopexport SPARK_HOME=/opt/spark-2.2.0-bin-hadoop2.7export SPARK_JAR=$SPARK_HOME/jars/*.jarexport PATH=$SPARK_HOME/bin:$PATH 从属节点配置复制文件1cp conf/slaves.template conf/slaves 配置地址打开slaves文件，填写如下内容： 123node1的IPnode2的IPnode3的IP 测试我们用spark文件夹中自带的求Pi例子做测试 单机模式（本地模式）1bin/spark-submit --master yarn-client --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.2.0.jar 集群模式1bin/spark-submit --master yarn-cluster --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.2.0.jar 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本2.2.1 小结完成这篇的配置后，我们可以做大部分数据挖掘工作了，但数据只能使用csv文件，为了让我们可以使用数据库，快速查找，快速计算，快速存取。下篇文件我们开始《HBase 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 基础教程[系列]]]></title>
    <url>%2Fai%2Fhadoop-tutorial%2F</url>
    <content type="text"><![CDATA[前言这一系列的文章主要介绍，Hadoop基础，说明各个工具的作用及用途，相互之间的关系。 目录 HDFS MapReduce Zookeeper 关系架构 Spark HBase Jupiter]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 部署]]></title>
    <url>%2Fai%2Fhadoop-dfs%2F</url>
    <content type="text"><![CDATA[这篇文章我们将按照规划方案配置HDFS，从4台中任一选择一台进行配置，本文选择node0。 Quick Start下载及Java配置登陆官方网站，下载hadoop.tar.gz文件，本文所使用的版本为2.7.4，下载完成后解压并进入该文件夹，修改etc/hadoop/hadoop-env.sh文件 1JAVA_HOME=/opt/jdk1.8.0_65 注：因为Hadoop相关的工具比较多，可以把所有工具统一放在相同文件路径下，即使在不同服务器中也可以方便查找，本文将统一放在/opt路径下 HDFS配置配置etc/hadoop/hdfs-site.xml，将下面的XML标签项添加在&lt;configuration&gt;标签内。 注：配置项中需要填写IP地址的地方，强烈推荐填写IP地址，不要使用主机名。（在访问页面时，方便大家在不做hosts文件修改时，正常跳转） 服务名1234&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt; NameNode服务的名字1234&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt; NameNode的RPC协议与端口配置该项后，可以通过程序调用8020接口，RPC协议主要用于系统内部通信以及用户编程访问。 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node0的IP:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node1的IP:8020&lt;/value&gt;&lt;/property&gt; NameNode的HTTP协议与端口配置该项后，可以通过浏览器访问50070接口 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node0的IP:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node1的IP:50070&lt;/value&gt;&lt;/property&gt; 固定配置，客户端通过该类找到active的NameNode1234&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; SSH安全12345678&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt; JournalNode的地址与端口1234&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node1的IP:8485;node2的IP:8485;node3的IP:8485/mycluster&lt;/value&gt;&lt;/property&gt; JournalNode的工作目录1234&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;此处填写你希望保存的路径即可，本文放在 /opt/hadoop-2.7.4/journalnode.edits&lt;/value&gt;&lt;/property&gt; ZKFC自动切换1234&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 打开权限控制1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; slaves文件配置方式配置datanode时，如果不是使用了主机名加DNS解析或者hosts文件解析的方式，而是直接使用ip地址去配置slaves文件 1234&lt;property&gt; &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; CORE配置配置etc/hadoop/core-site.xml，将下面的XML标签项添加在&lt;configuration&gt;标签内。 NameNode入口1234&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt; ZooKeeper地址与端口1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0:2181,node2:2181,node3:2181&lt;/value&gt;&lt;/property&gt; NameNode原数据存储目录1234&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;可自定义设置，本文存储路径 /opt/hadoop-2.7.4/tmp&lt;/value&gt;&lt;/property&gt; 其他配置项指定DataNode地址在etc/hadoop文件下，创建slaves文件，内容如下： 注：此处可以填写IP地址，也可填写主机名，推荐IP地址，保持配置一致性 123node1的IPnode2的IPnode3的IP 分发工具因为Hadoop会使用到所有的服务器，所以你必须将它分发到你所有的机器节点上，本教程一共4台服务器，所以将Hadoop文件夹分发到其他3台。 123scp -r /opt/hadoop-2.7.4 root@node1:/optscp -r /opt/hadoop-2.7.4 root@node2:/optscp -r /opt/hadoop-2.7.4 root@node3:/opt 启动JournalNode按照规划我们并没有把JournalNode服务部署在所有服务器节点上，所以，这里需要分别启动node1，node2，node3上的JournalNode进程。 1sbin/hadoop-daemon.sh start journalnode 使用jps命令查看是否启动成功，显示PID JournalNode则为成功。 格式化NameNode就像我们新装windows操作系统一样，需要磁盘格式化，从而建立该系统的元数据。 在node0与node1之间，选择任一选择（这里选择node0），运行如下命令： 1bin/hdfs namenode -format 格式化成功后会在tmp/dfs/name/current/目录下生成fsimage元数据 启动NameNode服务注：启动node0节点上的NameNode服务，目的是拷贝刚刚格式化好的元数据到node1中。 1sbin/hadoop-daemon.sh start namenode 注：如果启动失败，可以删除之前生成的元数据，重新格式化。 拷贝元数据将node0中的NameNode服务正常启动后，就可以拷贝元数据到node1中了。 注：下面这条命令必须在node1中执行。 1bin/hdfs namenode -bootstrapStandby 查看拷贝是否成功。可在node1中tmp/dfs/name/current/目录下查看是否生成fsimage元数据。 注：如果格式化NameNode与拷贝元数据这几步中依然出现莫名的错误，可以删除2个节点上的元数据，重新选择另一台机器（这里选择node1）从格式化NameNode步骤开始，再做一遍。（笔者之前就遇到过此类莫名其妙的问题- -!） 格式化DFSZKFailoverController 进行此步之前，需要关闭所有dfs服务： 1sbin/stop-dfs.sh 格式化ZKFC: 1bin/hdfs zkfc -formatZK 启动HDFS服务完成以上步骤后，就可以启动HDFS服务了，在4台节点中，任一选择一台键入命令，都可以启动所有节点的服务。这里推荐使用node0。 1sbin/start-dfs.sh 访问与测试正常启动HDFS服务后，再node1中使用jps命令看到NameNode，JournalNode，DFSZKFailoverController，DataNode服务。 访问HTTP服务通过IP地址:50070接口，在浏览器中正常访问到HDFS。 注：node0与node1中，一台是active状态，一台是standby状态，由ZooKeeper服务投票决定。 手动切换如果启动HDFS时，两个NameNode都处于standby状态，我们也可以手动指定一台节点为激活状态。本文指定nn2（这里使用配置项中的NameNode服务名） 1bin/hdfs haadmin -transitionToActive --forcemanual nn2 active状态：transitionToActive，standby状态：transitionToStandby 测试 创建test目录 1bin/hdfs dfs -mkdir -p /test 上传hello.txt文件到test目录 1bin/hdfs dfs -put hello.txt /test/ 此时可以在web页面中，查看刚刚创建的文件夹在Utilities -&gt; Browse the file system下查看 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结完成上述配置后，HDFS可以正常访问了，HDFS很多操作能够正常使用，MapReduce是必不可少的。随着HDFS的配置完成，说明MapReduce也配置完成。下篇文件我们开始《YARN 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN 部署]]></title>
    <url>%2Fai%2Fhadoop-yrn%2F</url>
    <content type="text"><![CDATA[上一篇文章我们已经完成了HDFS系统的部署，接下来我们开始YARN的配置，它是资源调度很重要的部分。依然选择在node0节点上进行配置。 Quick StartMapReduce 资源调度配置Hadoop中计算引擎的运行方式有很多，在企业级应用中我们选择yarn作为资源调度的方式。 配置MapReduce的资源调度方式复制etc/hadoop/mapred-site.xml.template为mapred-site.xml，并添加如下配置项： 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; YARN 配置在etc/hadoop/yarn-site.xml文件中，添加如下配置项： 资源申请的主机1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node0的IP&lt;/value&gt;&lt;/property&gt; NodeManager 附属服务1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 服务类（固定配置）1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt; 注：下面打问号的6个配置项，需要计算，才能得出（依据自己机器的节点数，每个节点的具体硬件配置，以及各自的任务需要）。这也是yarn配置的重点。如果想了解YARN在不同数量及配置的服务器中该如何计算，请点击此处，进行查看。 内存总量1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 最小可申请内存量1234&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; yarn启动时分配给AppMaster的默认内存大小1234&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; JVM参数1234&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 可供调用的CPU线程数这个配置项指你分配多少个CPU线程供yarn调度，可以全部，也可以分配一部分，主要看这个节点的想如何使用。 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 单个任务可申请的最多CPU线程数1234&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 计算好数值，将问号处填写好后，yarn的配置就完成了。 启动服务如果你之前已经启动了HDFS服务，这里只需系统yarn服务就可以了。 1sbin/start-yarn.sh 如果未启动，可以使用如下命令，启动Hadoop全部服务。 1sbin/start-all.sh 测试可以安装如下命令格式，提交编写好的jar包，及数据提交给MapReduce进行计算。 bin/hadoop jar [x.jar] [hdfs://数据所在目录] [hdfs://结果导出目录] 结果返回到指定hdfs目录。结果集可以使用hdfs命令行查看，也可取回本地查看。 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结配置好YARN之后，我们就可以编写代码利用MapReduce完成基本的数据挖掘工作了，但MapReduce作为离线计算框架，在速度方面并不能让我们满意，我们需要更快速更灵活的计算框架。下篇文件我们开始《Spark 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 部署]]></title>
    <url>%2Fai%2Fhadoop-zkp%2F</url>
    <content type="text"><![CDATA[ZooKeeper是分布式应用程序协调服务，在分布式系统中必不可少。它是为分布式应用提供一致性服务的软件，所以我们首先来安装配置它。 安装前，我们需要先准备好安装包，点击官方下载地址，本文所使用的版本是3.4.10 Quick Start创建目录 在解压后的文件夹中创建一个名为tmp文件夹，作为其工作目录。 再创建一个名为zk_data文件夹，作为其数据存储目录。 配置 拷贝conf/zoo_sample.cfg文件，并重命名zoo.cfg （这里必须命名为zoo.cfg） 修改zoo.cfg文件，内容如下： 12345678tickTime=2000dataDir=/opt/zookeeper-3.4.10/zk_dataclientPort=2181initLimit=5syncLimit=2server.1=node0的IP:2888:3888server.2=node2的IP:2888:3888server.3=node3的IP:2888:3888 将此配置分别配置到 node0， node2， node3中 按照规划ZooKeeper会被部署在node0，node2，node3上，所以需要在这三台服务器中都拷贝一份。 12scp -r /opt/zookeeper-3.4.10 root@node2:/optscp -r /opt/zookeeper-3.4.10 root@node3:/opt 分别在这三个节点的dataDir指向的目录下创建myid文件，值分别为1，2，3 在node0，node2，node3中依次启动服务，命令如下： 1bin/zkServer.sh start 查看服务是否已经启动 1zkserver.sh status 如果遇到Error contacting service. It is probably not running 此错误，请查看对应版本的官方文档重新配置zoo.cfg。 如果是java.net.BindException: Address already in use 通过netstat -nltp | grep 2181检查是否该端口已被占用。 如果，遇到防火墙原因，请继续往下看。 关闭防火墙​关闭所有服务器的防火墙（重点） firewall 查看防火墙状态 1firewall-cmd --state 关闭防火墙 1systemctl stop firewalld.service 禁止开机启动 1systemctl disable firewalld.service 关闭iptables 1service iptables stop 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结完成上述配置后，ZooKeeper应该可以正常启动了，下篇文件我们开始《部署 HDFS》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式平台前期规划]]></title>
    <url>%2Fai%2Fhadoop-planning%2F</url>
    <content type="text"><![CDATA[完成上一篇文章《服务器批量安装》的内容后，我们已经拥有了4台Linux服务器，且相互之间网络可以互通，并且正常运行SHH服务。硬件环境已经准备完成，这篇文章我们将开始讲述Hadoop前期规划的准备工作。Hadoop是一系列工具的集合，如何合理的规划这些工具以及分配服务器资源，是一个非常重要的工作。 Quick Start主机名配置我将分别修改主机名为node0，node1，node2，node3。方便教程的讲述，也方便ssh中的操作。选择其中一台服务器，root用户登陆。 查看主机名1hostname 修改主机名 方法一：修改network文件，将HOSTNAME后面的值改为node0，重启后生效。 1vim /etc/sysconfig/network 方法二：修改当前的主机名，立即生效。 1hostname node0 配置hosts文件修改/etc/hosts文件，ip0为你自己机器的ip地址 1234ip0 node0ip1 node1ip2 node2ip3 node3 分发hosts文件将hosts文件分发到其他3台机器中，以保证所有服务器识别主机名。 123scp /etc/hosts root@node1:/etc/scp /etc/hosts root@node2:/etc/scp /etc/hosts root@node3:/etc/ 批量管理工具推荐如果你想更快，更省力的完成批量操作，有兴趣的童鞋可以安装Linux集群批量管理工具parallel-ssh(PSSH)，该工具需要Python环境，安装及操作点击此处链接，该教程中还是采用普通命令进行讲述。 配置免密码登录之后的服务器命令都需要免密码才能正常操作，这是一个必须而重要的步骤。我以node0批量操作其他服务器为例。此步骤需要在所有服务器中完成一边，以方便任意两台机器可以互相登陆。 创建本机的公钥与私钥12cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 分发公钥到其他3台服务器我以node1为例 1scp ~/.ssh/id_rsa.pub root@node1:~/ 公钥追加进入node1的root账户的home目录下，运行如下命令。完成后，就可以从node0免密码登录到node1了。 1cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 安装Java环境hadoop整套工具都以Java环境为基础，所以4台机器都需要安装。我们以node0为例。 查看是否安装1syum list installed | grep java 查看yum库中的Java安装包1yum -y list java* 安装Java我们以版本1.8.0为例 1yum -y install java-1.8.0-openjdk* 配置环境变量 将jdk文件夹移动到opt文件夹下 在/etc/profile文件中追加如下内容： 12export JAVA_HOME=/opt/jdk1.8.0_65export PATH=$JAVA_HOME/bin:$PATH JSP进程集工具集介绍在此篇基础工具集的规划中，我们主要安装Hadoop, ZooKeeper, HBase, Spark, Jupiter, Thrift。之后的教程中还会讲到Hive，MySQL等。每种工具都对应着一些Java进程，我们将规划这些进程分别部署到哪个服务器上。（话说，分布式应用，总不能把所有的进程都安装在一台服务器中吧。。。 - -!） 注：如果你对上述工具还不熟悉，请跳转到《Hadoop 基础教程》 JSP进程JSP是Java Virtual Machine Process Status Tool的缩写，在JVM中所有具有访问权限的Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与Linux上的ps命令类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 工具 JPS进程 ZooKeeper QuorumPeerMain HDFS NameNode, DataNode, JournalNode, ZKFailoverController MapReduce, Spark ResourceManager, NodeManager HBase HMaster, HRegionServer Thrift ThriftServer 系统进程 工具 系统进程 Jupyter jupyter-notebook 进程规划规划图根据上一篇的教程，我们准备了4台服务器，我们将把上面介绍的进程部署在这4台服务器中，方案如下： 图中勾选的位置对应着进程将部署在哪台服务器。 缩写对照表 缩写 进程全称 NN NameNode DN DataNode ZK QuorumPeerMain ZKFC ZKFailoverController JN JournalNode RM ResourceManager NM NodeManager HM HMaster HR HRegionServer TS ThriftServer 小结此篇主要介绍平台安装前的准备工作，以及要部署的工具集与JSP进程的规划方案，当然这个规划方案是以4台服务器为基础，如果你的服务器数量超过4台（无论怎样要大于等于3台，原因可以在《Hadoop 基础教程》中了解，此处不在赘述！）规划方案可以相应调整，下篇《ZooKeeper 部署》。 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KOA 开始]]></title>
    <url>%2Fkoa%2Fkoa-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>koa</category>
      </categories>
      <tags>
        <tag>NodeJS</tag>
        <tag>全栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器批量安装]]></title>
    <url>%2Fai%2Fhadoop-servers%2F</url>
    <content type="text"><![CDATA[在安装Hadoop分布式系统之前，我们需要准备好服务器资源，如果采用云服务器，可以跳过此篇文章。批量无人值守安装操作系统，此次示例系统为CentOS7.x且推荐安装X window用户界面，后面会用到，服务器数量为4台，当然你可以使用大于等于3台以上数量的机器。 在4台机器中，随意选择1台安装服务。注：推荐直接使用实体机进行安装，或者非VMware的虚拟机，否则无人值守批量安装系统时可能会出错。 Quick Start安装FTP服务安装vsftp服务：1yum -y install vsftpd 启动vsftp服务：1systemctl start vsftpd.service 将准备好的系统iso文件加载到光驱如果使用虚拟机，则加载iso文件，如果是实体机可使用光盘或U盘加载系统文件。 将光驱文件挂在到ftp目录下：1mount /dev/cdrom /var/ftp/pub 测试FTP服务是否可以匿名登录，命令如下： 如果系统提示lftp服务未安装，安装lftp 1yum -y install lftp 进入lftp模式后会看到pub文件夹，如果没有，请关闭防火墙和selinux 关闭防火墙：systemctl stop firewalld.service查看selinux：getenforce暂时关闭selinux：setenforce 0永久关闭selinux：修改/etc/selinux/config文件SELINUX=enforcing改为SELINUX=disabled，重启即可 进入pub文件夹，如果有文件，测试正常 安装PXE并生成pxelinux.0启动文件安装syslinux服务：1yum install -y syslinux 查询文件所在目录：1rpm -ql syslinux | grep "pxelinux.0" 安装TFTP服务安装tftp服务：1yum -y install tftp-server 修改配置文件：打开/etc/xinetd.d/tftp文件，修改disable=no 启动tftp服务：1systemctl start xinetd.service 查看server_args的值找到tftpboot文件夹路径（通常为/var/lib/tftpboot）1systemctl start xinetd.service 拷贝文件在tftpboot文件夹下，新建文件夹pxelinux.cfg，并执行如下命令：1234cp /usr/share/syslinux/pxelinux.0 .cp /var/ftp/pub/isolinux/isolinux.cfg ./pxelinux.cfg/defaultcp /var/ftp/pbu/isolinux/vmlinuz .cp /var/ftp/pbu/isolinux/initrd.img . 设置权限设置./pxelinux.cfg/default的权限为644： 1chmod 644 default 安装DHCP服务安装dhcp服务1yum -y install dhcp 配置/etc/dhcp/dhcpd.conf：123456789101112131415ddns-update-style interim;allow booting;allow booting;next-server 192.168.0.1;filename "pxelinux.0";default-lease-time 1800;max-lease-time 7200;ping-check true;option domain-name-servers 192.168.0.1;subnet 192.168.0.0 netmask 255.255.255.0&#123; range 192.168.0.100 192.168.0.220; option routers 192.168.0.1; option broadcast-address 192.168.0.255;&#125; 启动HDCP：1systemctl start dhcpd.service 安装Kickstart工具安装:1yum -y install system-config-kickstart 运行Kickstart并配置选项启动Kickstart Configurator界面（该软件需要系统安装X window）1system-config-kickstart 配置选项页Basic Configuration： Time Zone设置为Asia/Shanghai 勾选 Use UTC clock 设置Root Password与Confirm Password 勾选Reboot system after installation 配置选项页Installation Method： 在Installation source选框中 点选 FTP 填写FTP Server： 192.168.0.1 填写FTP Directory： pub 配置选项页Boot Loader Options： 点选 Install new boot loader 配置选项页Partition Information 勾选 Clear Master Boot Record 勾选 Remove all existing partitions 勾选 Initialize the disk label 点击Add 自定义分区 创建分区 新增 /boot分区 文件系统类型xfs或者ext4 Fixed size: 200MB 新增 /swap分区(在File System Type中选择) Fixed size: 2048MB 新增 / 分区 点选Fill all unused space on disk 创建完成后点击OK 配置选项页Network COnfiguration：点击Add Network Device, 下拉菜单中选择DHCP, 如果Network Device为空，请填写自己的网卡设备 配置选项页Fireswall Configuration： SELinux下拉选项：Disabled Security level下拉选项：Disable firewall 保存选项到文件完成配置并保存到/var/ftp/ks/ks.cfg 修改启动引导文件文件路径为/var/lib/tftpboot/pxelinux.cfg/default 1234timeout 60 //暂定时间label ks //选项kernel vmlinuzappend ks=ftp://192.168.0.1/ks/ks.cfg initrd=initrd.img 类似如下图： 小结到此，无人值守服务已全部配置完成，分别开启其他3台机器后，可自动进入系统安装。下篇《分布式平台前期规划》。 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React Native 开始]]></title>
    <url>%2Freact%2Frn-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>react</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 系统搭建[系列]]]></title>
    <url>%2Fai%2Fhadoop-start%2F</url>
    <content type="text"><![CDATA[前言这一系列的文章主要介绍，Hadoop分布式系统如何从硬件到软件搭建完成，相关插件的开发及使用的教程 目录 服务器批量安装 分布式平台前期规划 ZooKeeper 部署 HDFS 部署 YARN 部署 Spark 部署 HBase 部署]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用监控]]></title>
    <url>%2Fother%2Flinux-monitor%2F</url>
    <content type="text"><![CDATA[知道程序在服务器中运行时，对硬件的使用率有多少，尤其是长时间，大规模的运算任务，还是一件很重要的事。这篇文章简单介绍几个性能分析工具。 Quick StartCPU及内存监控top命令是Linux下常用的性能分析工具，能够实时显示CPU的使用率及系统中各个进程的资源占用状况。以每秒更新的方式显示实时情况。 基础信息1top 基础操纵点击下面的健会显示相应信息，点击ESC返回主界面 h: 帮助1: 显示CPU详细信息，每行代表一个线程f: 查看列信息n: 按下n键后，再按相应的数字，表示现实进程前几行z: 颜色模式q: 退出。或Ctrl+C 显示与隐藏l: 第一行负载信息t: CPU线程信息m: 内存信息 显卡监控如下命令主要适用于NVIDIA产品 1nvidia-msi IO监控基础方法每隔1秒采样一次，以KB的方式显示。 1iostat -d -x -k 1 如果显示有限次数，则在命令最后增加一个次数，下面命令显示5次 1iostat -d -x -k 1 5 参数说明d: 采样x: 详细信息，使用率c: CPU状态]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 硬件及系统信息查询]]></title>
    <url>%2Fother%2Flinux-info%2F</url>
    <content type="text"><![CDATA[最近在做一些与服务器硬件相关的事情，总是需要查询服务器的硬件信息（Linux上查询这些信息的确没有Windows上简单直观，且很多命令对我来说并不常用，完全记不住啊… - -!） 这文章，帮助自己下次查询，有用的上的朋友也可以look look ：） Quick Start主板基础信息1dmidecode | more 内存次插槽数，及每个槽位内存条大小1dmidecode|grep -P -A5 "Memory\s+Device"|grep Size|grep -v Range 查看最大支持内存容量1dmidecode|grep -P 'Maximum\s+Capacity' CPU基础信息1cat /proc/cpuinfo 物理CPU个数1cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l 单个物理CPU的核数1cat /proc/cpuinfo| grep "cpu cores"| uniq 单个物理CPU的线程数（逻辑CPU）1cat /proc/cpuinfo| grep "processor"| wc -l 是否开启超线程 逻辑CPU &gt; 物理CPU x CPU核数 （已开启超线程） 逻辑CPU = 物理CPU x CPU核数 （未开启超线程或不支持超线程） 内存基础信息1cat /proc/meminfo 内存使用及缓存信息1free 硬盘基础信息1fdisk -l 磁盘分区1df 外设键盘鼠标1cat /proc/bus/input/devices PCI基础信息1lspci 查看系统版本查看内核版本1cat /proc/version 查看发行版本1cat /etc/issue 查看issue有些系统返回为空，如果是redhat系列的版本可用如下命令查看： 1cat /etc/redhat-release]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NBatis 开始]]></title>
    <url>%2Fnbatis%2Fnbatis-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>nbatis</category>
      </categories>
      <tags>
        <tag>NodeJS</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReactJS 开始]]></title>
    <url>%2Freact%2Freactjs-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>react</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>SPA</tag>
      </tags>
  </entry>
</search>
