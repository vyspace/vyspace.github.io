<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CSV导入HBase]]></title>
    <url>%2Fai%2Fhadoop-cth%2F</url>
    <content type="text"><![CDATA[大数据分析中，都会用到csv数据文件，这篇文章我们讲讲如何将csv中的数据导入到hbase中，方便数据的查看，计算。也方便在后续的程序中可以直接从数据库中读取数据。 下面的程序实现了如下功能： 将csv文件从本地上传到hdfs文件系统中。 在hbase中创建表。 把hdfs中的csv数据导入新建的表中。 执行每一步，打印出提示信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165import osimport sysimport subprocessfrom thrift import Thriftfrom thrift.transport import TSocket, TTransportfrom thrift.protocol import TBinaryProtocolfrom hbase import Hbasefrom hbase.ttypes import *# 环境变量获取hadoop_home = os.environ.get('HADOOP_HOME')# jupyter_notebook 工作区根目录jn_home = os.environ.get('JN_HOME')hbase_home = os.environ.get('HBASE_HOME')# ============用户参数设置开始============file_name = 'csv文件名'table_name = '要新建的表名'# hdfs中csv已存在选项# 1: 跳过此处，不再上传文件，但继续执行后面的代码。# 2: 停止执行后面的代码。# 3: 删除已存在的文件，重新上传后，继续执行后面的代码。hdfs_exist_flag = 3 # 表存在选项# True: 删除已有表，创建新表。# False: 程序停止。用户自己决定是否手动删处理。hbase_exist_flag = True# ===============设置完成================# 参数设置检测if file_name == '' or table_name== '': print '参数设置不能为空' sys.exit(1)# 检测本地文件是否存在# 可以指定任意本地路径，这里我把所有数据文件放在jupyter工作根目录的Local_Data下，方便管理及多人共享。local_file_path = '%s/Local_Data/%s' % (jn_home, file_name)if os.path.exists(local_file_path): print '本地文件存在, 正在检测HBase'else: print '本地文件不存在' sys.exit(1)# 检测HBase中表是否存在hm_ip = 'node3的IP'hm_port = 9090transport = TSocket.TSocket(hm_ip, hm_port)transport = TTransport.TBufferedTransport(transport)protocol = TBinaryProtocol.TBinaryProtocol(transport)client = Hbase.Client(protocol)transport.open()# 试图创建表，如果已存在，则会跳入异常try: print "试图创建表" # 创建表时，把cf列族的版本数设为1 contents = ColumnDescriptor(name='cf:', maxVersions=1) client.createTable(table_name, [contents])except: if hbase_exist_flag: print "表[%s]已存在，正在删除表" % table_name # hbase中的表，停用后才能删除 client.disableTable(table_name) client.deleteTable(table_name) contents = ColumnDescriptor(name='cf:', maxVersions=1) client.createTable(table_name, [contents]) # 创建成功后，打印出所有表名 print "表创建成功，列表如下：" table_name_list = client.getTableNames() for line in table_name_list: print line.strip() transport.close() else: print '表[%s]已经存在，请重新命名' % table_name sys.exit(1)else: print "表创建成功，列表如下：" table_name_list = client.getTableNames() for line in table_name_list: print line.strip() transport.close()hdfs_file_path = '/hfile/data/%s' % file_name# 导入hbasedef hbaseImport(): print "开始导入数据" # 按照csv中的字段定义导入表带规则 # 此时字段1将被用做HBASE_ROW_KEY rule = 'HBASE_ROW_KEY,cf:字段2,cf:字段3, ...' # 这里需要使用到hadoop提供的ImportTsv类，帮助我们进行导入。 cmd = '%s/bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="," -Dimporttsv.columns=%s %s %s &amp;&amp; echo $?' % (hbase_home, rule, table_name, hdfs_file_path) p = subprocess.Popen(cmd, shell=True, stdout = subprocess.PIPE ) out = p.stdout.readlines() # 导入成功后，打印第一条纪录 if len(out) &gt; 0: print "导入成功, 第一条内容如下：" transport.open() id = client.scannerOpen(table_name, '1', None) result = client.scannerGetList(id, 1) client.scannerClose(id) transport.close() for line in result: print line else: print "导入失败" sys.exit(1)# 上传csvdef uploadHDFS(): print "开始文件上传" cmd = '%s/bin/hadoop fs -put %s /hfile/data' % (hadoop_home, local_file_path) p = subprocess.Popen(cmd, shell=True, stderr = subprocess.PIPE ) out = p.stderr.readlines() if len(out) &gt; 0: for line in out: print line.strip() sys.exit(1) else: print "上传成功, 试图导入数据" hbaseImport()# 删除hdfs中的文件def deleteHDFS(): print "文件已存在，开始删除文件[%s]" % file_name cmd = '%s/bin/hadoop fs -rm -f %s &amp;&amp; echo $?' % (hadoop_home, hdfs_file_path) p = subprocess.Popen(cmd, shell=True, stdout = subprocess.PIPE ) out = p.stdout.readlines() if len(out) &gt; 0: print "删除成功, 试图上传文件" uploadHDFS() else: print "删除失败" sys.exit(1)# 测试hdfs中指定的文件是否存在 def testHDFS(): cmd = '%s/bin/hadoop fs -test -e %s &amp;&amp; echo $?' % (hadoop_home, hdfs_file_path) p = subprocess.Popen(cmd, shell=True, stdout = subprocess.PIPE ) out = p.stdout.readlines() if len(out) &gt; 0: if hdfs_exist_flag == 1: print "文件已存在, 试图导入数据" hbaseImport() elif hdfs_exist_flag == 2: print "文件已存在，请删除HDFS文件或重命名后，重新执行" sys.exit(1) elif hdfs_exist_flag == 3: deleteHDFS() else: print "hdfs_exist_flag参数设置有误" sys.exit(1) else: uploadHDFS() testHDFS()sys.exit(0) 执行完成后，会在Jupyter中打印如下信息： 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计HBase表纪录数]]></title>
    <url>%2Fai%2Fhadoop-hnc%2F</url>
    <content type="text"><![CDATA[我们知道所有HBase中的表数据，都存放在分布式文件系统上，所以要获得到一张表中到底有多少条记录，跟传统关系型数据库是不一样的，这篇文章我们来说说如何获得。 我们依然使用Jupyter Notebook，用程序的方式获取结果。 1234567891011121314import osimport subprocesshbase_home = os.environ.get('HBASE_HOME')# 你想查看的表名table_name = '表名'if table_name != '': cmd = '%s/bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter %s' % (hbase_home, table_name) p = subprocess.Popen(cmd, shell=True, stderr = subprocess.PIPE ) out = p.stderr.readlines() for line in out: print line.strip()else: print '定义表名' 我们将结果直接打印在Jupyter中，如下图： 红框中便是我们所得到的结果。 在这段程序中，我们使用了HBase提供的MapReduce计算类RowCounter，所以当执行这段程序时，其实是提交一个MapReduce计算任务。在Hadoop相关的软件中，会大量使用MapReduce做一些统计任务。 当然你也可以在命令行中，执行这条命令： 1bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter 表名 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word Count]]></title>
    <url>%2Fai%2Fhadoop-wct%2F</url>
    <content type="text"><![CDATA[Word Count程序就像初学每种语言时要写的Hello World一样。他可以用来统计大段文章（可能是一本朗文字典那么厚的书）中相同单词出现的次数。当然除了统计相同词数量外，我们经常拿它来统计网站中大量的PV，UV。 所有的Spark程序可以在local模式下运行，也可以在cluster模式下运行。因为是第一次应用，我们会分别讲解这两种模式。以后的程序基本都是运行在集群模式。 Spark程序可以使用Python，Scala，Java，R编写，推荐程度Python最高，R最低。按道理说整个Spark都是Scala开发的，使用Scala是最好的选择，的确是这样的。但是由于Python在整个机器学习领域的热度越来越热，为了方便大家之后开发其他程序（比如模式识别，语音识别等等），这里推荐使用Python，当然其他语言版本我们会大概讲下，本文我们看看Python，Scala，Java三种语言分别是如何实现World Count程序的，废话不多说，下面上主菜… Quick StartPython注：本文使用的语言是Python2.7，编辑器Jupyter Notebook 本地模式123456789101112131415161718192021222324252627282930313233343536import osfrom pyspark import SparkConf, SparkContext# 准备好的数据本地路径input_file = "file:///home/test/jupyter_notebook/wordcount/data_wc_py"# 计算完成后统计结果存放路径out_dir = "file:///home/test/jupyter_notebook/wordcount/wjh"# 判断spark上下文是否已释放try: sc.stop() except: pass # 设置spark运行模式以及任务名称conf = SparkConf().setMaster("local").setAppName("word_count_local")# 按照设置，获取sprak上下文sc = SparkContext(conf=conf)# 按行读取数据，生成类数组lines = sc.textFile(input_file)# 这里使用了lambda表达式# 第一个表达式，将每行按空格把行转换成单词# 第二个把每一个单词转换成元组（单词，1）# 最后一个将单词相同的元组做累加# 最后把（单词，累加值）返回给counts数组counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)# 把最后的结果集保存在文件中counts.saveAsTextFile(out_dir) # 释放spark上下文sc.stop() 注：只有本地模式才能在计算后，直接访问到存储结果集文件。 集群模式在讲这段程序前，讲点其他有的没的。首先，Jupyter是一个网页程序，尤其团队使用时，它被不同的人同时使用，所以为了代码管理需要，每个人可以在Jupyter的根目录建立自己的工作文件目录，自己编写的程序都放在自己工作目录下，这样可以有效管理自己的代码。 第二，集群模式下运行时，我们可以在用8088端口访问浏览器，查看我们提交的计算任务进度，计算日志等。如下图，我们只要提交任务，都可以在这里看到，蓝框中是你为本次任务设置任务名，我不推荐这样命名，之前说过了因为同时间会有不止一人提交任务，如果大家都按程序目的命名，你很难分辨哪个才是你自己提交的。所以我推荐使用自己的名字命名，当然你可以用（姓名＋目的名＋时间戳）命名也不错，为什么要加时间戳，因为同一个任务可能程序问题，会提交不止一个版本。 第三，你可以对应不同的计算引擎（MapReduce, Spark, Storm等），编写不同的代码，提交后都可以在这里看到。 注：集群模式下提交任务后，无法停止，要么等运行结束，要么报错结束。所以小规模测试时可以使用本地模式，等程序调试基本完成，可以提交集群模式，运行大规模数据版本。 1234567891011121314151617181920212223242526272829303132333435363738394041import osimport timefrom os import path from pyspark import SparkConf, SparkContext# 生成当前时间戳time_now = int(time.time())# 获取程序所在的父目录路径dir_path = os.getcwd()# 获取父目录名，因为个人的工作目录以自己的名字命名（英文或拼音）dir_name = path.basename(dir_path)# 获取数据文件在HDFS中的路径server_input_file = '/hfile/data/data_wc_py'# 运行集群模式，所以资源调度交给yarnmaster = 'yarn'# 之前提到过的任务名设置job_name = '%s_%s' % (dir_name, time_now)try: sc.stop() except: pass conf = SparkConf().setMaster(master).setAppName(job_name)sc = SparkContext(conf=conf)lines = sc.textFile(server_input_file)counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)# 这次我们选择将结果直接打印出来# 因为Spark程序计算后依然是RDD，我们需要用collect将其转化为类数组result = counts.collect()# 打印前100个结果print result[:100]sc.stop() Scala注：本文使用的语言是Scala2.1，编辑器Scala Eclipse，官网提供 123456789101112131415161718package org.test.wctimport org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConfobject WordCount &#123; def main(args: Array[String]): Unit = &#123; val inputFile = "数据文件路径" val outFile = "输出结果路径" val conf = new SparkConf().setAppName("任务名") val sc = new SparkContext(conf) val textFile = sc.textFile(inputFile) val wordCount = textFile.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_) wordCount.saveAsTextFile(outFile) sc.stop() &#125;&#125; Scala程序非常简洁，它提供了很多API，能够快速有效的帮助我们完成代码，编写Spark相关的程序，个人还是非常推荐的。 编写好程序后，就可以像Java一样，打包成jar文件，提交给Spark进行计算了。如果忘记了如何提交任务，请查看《Spark 部署》 Java注：Java1.8，编辑器Spring Tool Suite 最后我们编写word count的Java版本，这次我们使用MapReduce作为计算引擎。 Java版本的文件会稍微多一些，文件结构如下图所示： Map程序在WordCountMapper.java中编写如下代码： 1234567891011121314151617181920212223package org.test.mpr;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; String line = value.toString(); StringTokenizer st = new StringTokenizer(line); while(st.hasMoreTokens()) &#123; String word = st.nextToken(); context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; Reduce程序在WordCountReducer.java中编写如下代码： 1234567891011121314151617181920package org.test.mpr;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; iterable, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; int sum = 0; for(IntWritable i:iterable) &#123; sum = sum + i.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; 提交程序在JobRun.java中编写如下代码： 1234567891011121314151617181920212223242526272829package org.test.mpr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class JobRun &#123; public static void main(String[] args) &#123; Configuration conf = new Configuration(); try &#123; Job job = Job.getInstance(conf, "任务名"); job.setJarByClass(JobRun.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path("数据文件路径")); FileOutputFormat.setOutputPath(job, new Path("计算结果路径")); System.exit(job.waitForCompletion(true) ? 0: 1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 完成编写后，依然打jar，提交即可。 代码中看到的Writable类型，由hadoop提供，它跟普通的Int，String类型没有太多区别，你可以将它理解为分布式的整型，字符型。有兴趣的朋友可以查看官网文档。 小结这篇文章我们介绍了如果用不同的语言在两种计算引擎上，完成同一个任务。用户可以对比来看，那种语言更适合你。 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 使用[系列]]]></title>
    <url>%2Fai%2Fhadoop-use%2F</url>
    <content type="text"><![CDATA[前言这一系列的文章主要介绍，HDFS，Spark，HBase在Jupyter中该如何使用，如何编写我们的程序。 目录 Word Count 统计HBase中表纪录数 CSV导入HBase]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 附录]]></title>
    <url>%2Fai%2Fhadoop-add%2F</url>
    <content type="text"><![CDATA[Quick Start环境变量以下是各个节点的环境变量配置（/etc/profile） 节点node012345678910111213export JAVA_HOME=/opt/jdk1.8.0_65export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport ANACONDA2_HOME=/opt/anaconda2export PATH=$ANACONDA2_HOME/bin:$PATHexport SPARK_HOME=/opt/spark-2.2.0-bin-hadoop2.7export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATHexport XDG_RUNTIME_DIR="/home/test/.jupyter"export HBASE_HOME=/opt/hbase-1.3.1export HADOOP_HOME=/opt/hadoop-2.7.4export JN_HOME=/home/test/jupyter_notebookexport LD_LIBRARY_PATH=/opt/xgboost_packages/libhdfs 其他节点123456789export JAVA_HOME=/opt/jdk1.8.0_65export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport ANACONDA2_HOME=/opt/anaconda2export PATH=$ANACONDA2_HOME/bin:$PATHexport SPARK_HOME=/opt/spark-2.2.0-bin-hadoop2.7export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATHexport XDG_RUNTIME_DIR="/home/test/.jupyter"]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thrift 部署]]></title>
    <url>%2Fai%2Fhadoop-thr%2F</url>
    <content type="text"><![CDATA[Thrift服务是帮助Jupyter Notebook访问HBase，安装后我们就可以在程序中直接访问HBase了。 Quick Start下载安装按照之前的规划表，我们会在node3中启动Thrift服务，所以在这篇文章我们选择在node3中进行配置安装。 登陆官方网站下载Thrift安装包，版本0.10.0，下载完成后解压并进入该文件夹。 运行如下命令进行安装： 12345678# 配置./configure --with-cpp --with-boost --with-python --without-csharp --with-java --without-erlang --without-perl --with-php --without-php_extension --without-ruby --without-haskell --without-go# 编译make# 安装make install 启动服务安装完成后，进入/opt/hbase-1.3.1，启动thrift服务 1bin/hbase-daemon.sh start thrift 安装依赖包123sudo pip install thriftsudo pip install hbase-thrift 到此Thrift已安装完成，是不是很简单 : ） 测试下面我们来写段简单的Python(2.7)程序，测试HBase是否联通。这段程序获取HBase下的所有表名，结果会以类数组的方式打印出来。 12345678910111213141516171819from thrift import Thriftfrom thrift.transport import TSocket, TTransportfrom thrift.protocol import TBinaryProtocolfrom hbase import Hbasefrom hbase.ttypes import * # 参数配置hm_ip = 'node3的IP地址'# 这里的9090是HBase的RPC协议端口hm_port = 9090transport = TSocket.TSocket(hm_ip, hm_port)transport = TTransport.TBufferedTransport(transport)protocol = TBinaryProtocol.TBinaryProtocol(transport)client = Hbase.Client(protocol)transport.open()tableName = client.getTableNames()print tableNametransport.close() 官方文档如果需要了解更详细的内容，请访问官方文档 小结完成上述配置后，我们可以通过Python程序访问HBase了。下篇《Hadoop 附录》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter 部署]]></title>
    <url>%2Fai%2Fhadoop-jpt%2F</url>
    <content type="text"><![CDATA[Jupyter Notebook 是一个Python在线编辑器，在机器学习领域很流行，调试代码也很方便。当然你可以选择其他编辑器。因为后面的示例会有一部分使用Python完成，所以这篇我们来部署Jupyter。 Quick StartAnaconda安装Anaconda是Python的版本管理工具，登陆官方网站下载安装包，其安装文件分为Python3.x与Python2.7版本。 首先选择一个主要Python版本，这里选择2.7，所以我们下载Anaconda(py2)，下载可直接运行sh文件进行安装。安装完成后可运行如下指令查看conda的信息： 1conda info 切换Python3运行如下命令，安装Python3 1conda create -n py3 python=3 版本切换回到py2 1source deactivate py3 进入py3 1source activate py3 除了Anaconda，你也可以安装它的mini版，Miniconda。 Anaconda包内已经包含了Jupyter，在装完anaconda应该会自动安装了Jupyter，下面就可以直接启动Jupyter服务了。 Jupyter注：启动Jupyter服务建议不要使用root账户，原因后面会说到，所以我们切换到其他账户，这里我切换到test账户： 1su test 生成配置文件1jupyter notebook --generate-config 生成后的文件会在~/.jupyter中 自动生成密钥因为服务开放后，所有人都可以访问，所以需要配置密码（此步也是必须的）。 1jupyter notebook password 运行此命令后，可输入两次密码，完成后会在~/.jupyter/upyter_notebook_config.json文件中生成一串token。 配置HTTP服务打开~/.jupyter/jupyter_notebook_config.py文件，修改如下配置项： 1234567891011121314# 容许所有IP可访问c.NotebookApp.ip = '*'# 初始化notebook工作区根目录，我在test帐户下，新建jupyter_notebook文件夹作为根目录c.NotebookApp.notebook_dir = u'/home/test/jupyter_notebook'＃ 是否打开浏览器立即启动c.NotebookApp.open_browser = False＃ 之前生成的tokenc.NotebookApp.password ＝ u'sha1:xxx'＃ 端口配置c.NotebookApp.port = 8888 运行服务新建.jupyter_out文件夹，将所有log日志记录在jupyter.log文件中。 1nohup jupyter notebook &gt;/home/test/.jupyter_out/jupyter.log 2&gt;&amp;1 &amp; 注：jupyter服务不是jps服务，所以需要使用ps命令查看 HTTP访问启动完成后，浏览器访问8888端口，就可以正常打开Jupyter了， 注：第一次登陆需要填写密码 注：之前有提到启动Jupyter服务，我建议切换到非root账户，原因是在Jupyter Notebook中用户可以新建命令行终端，你用什么账户启动服务，这里新建的终端将会以什么账户登录。所以为了控制用户权限及系统安全的考虑，不建议使用root账户去启动服务。 因为我们用test账户启动Jupyter服务，所以下图可以看到命令行窗口默认test用户登录。 Spark集群配置为了让我们的jupyter可以访问Sprak集群去计算任务，我们还需要配置PySpark，请点击此处查看具体配置教程 因为配置过后时间隔的太久，记得不是很清楚了，可以试试如下命令，启动集群 1nohup jupyter notebook --profile=pyspark &gt;/home/test/.jupyter_out/jupyter.log 2&gt;&amp;1 &amp; 如果使用root账户启动，命令中加 –allow-root 1nohup jupyter notebook --profile=pyspark --allow-root &gt;/home/test/.jupyter_out/jupyter.log 2&gt;&amp;1 &amp; 到这里Jupyter服务已部署完毕。 小结完成上述配置后，我们的IDE也部署完成了，不过还有一些小缺陷，就是Jupyter无法访问HBase。下篇文件我们开始《Thrift 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 部署]]></title>
    <url>%2Fai%2Fhadoop-hbs%2F</url>
    <content type="text"><![CDATA[完成之前的章节，我们已经将Hadoop集群与Spark计算引擎成功部署在4个节点中了。你可以使用Java或者Scala语言（这里推荐Scala）进行开发，并可以用Spark正常进行数据挖掘了。这章我们讲HBase的部署，基本与数据存储有关。 Quick Start下载安装按照之前的规划表，我们会在node3中启动HBase的主进程，在node2中启动备用进程，所以在这篇文章我们选择在node3中进行配置。 登陆node3节点，并下载HBase安装包，版本1.3.1，下载完成后解压（文件目录还是统一放在/opt路径下）并进入该文件夹。 基础配置打开conf/hbase-site.xml文件，在configuration标签中添加如下配置项： 启动集群模式1234&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; HDFS中设置HBase主目录1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;&lt;/property&gt; ZooKeeper集群地址1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0的IP, node2的IP, node3的IP&lt;/value&gt;&lt;/property&gt; ZooKeeper快照存储位置注：此项与zoo.conf中dataDir路径相同。 1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/zookeeper-3.4.10/zk_data&lt;/value&gt;&lt;/property&gt; 配置元数据存储节点HBase中的数据分为元数据（文件索引）与文件本身数据，文件数据由DataNode负责存取，元数据则由HRegionServer负责。按照规划表，我们会把元数据分布在4台节点中，所以我们需要在所有节点中部署HRegionServer，配置方法如下： 在conf文件夹中打开regionservers文件（如果未找到，新建即可）。添加如下内容： 1234node0的IPnode1的IPnode2的IPnode3的IP 配置环境变量1234567891011# Java环境变量是必不可少的。export JAVA_HOME=/opt/jdk1.8.0_65# 因为HBase自身就带有一个ZooKeeper，非集群模式时，我们可以用它自己带的就好，集群模式下关闭它，防止启动多个ZooKeeperexport HBASE_MANAGES_ZK=false# 让HBase可以找到Hadoop的配置文件hdfs-site.xml，这里配置目录路径就好。export HBASE_CLASSPATH=/opt/hadoop-2.7.4/etc/hadoop# HBase工作目录路径（tmp文件夹是我自己创建的，你也可以指定到别的路径下）export HBASE_PID_DIR=/opt/hbase-1.3.1/tmp 分发安装包将配置好的HBase文件夹拷贝到所有节点中 123scp -r /opt/hbase-1.3.1 root@node0:/optscp -r /opt/hbase-1.3.1 root@node1:/optscp -r /opt/hbase-1.3.1 root@node2:/opt 启动服务在node3上执行如下命令： 1bin/start-hbase.sh 执行完成后使用jps命令进行查看，node3中会有HMaster和HRegionServer服务 HA为了达到高可用，我们需要启动一个备用进程，按照规划图，在node2中运行如下命令： 1bin/hbase-daemon.sh start master Web 访问在浏览其中输入地址可以访问HMaster 访问HRegionServer 测试SHELL在命令行中输入如下命令，进入hbase shell界面后，可执行一些基础操作。 1bin/hbase shell 注：HBase的命令与其它数据库（例如：MySql）不同，命令行结束后不能加分号，名称（表名）要加引号（双引号或单引号） 创建表1create '表名', '列族' 查看所有表1list 查看表属性1describe '表名' 插入数据1put '表名', 'rowkey', '列族:列', '值' 查看表中所有数据1scan '表名' 以上是一些基础命令的测试，如果你对HBase的shell操作有更多的兴趣，请点击下方官方文档进行查阅。 官方文档如果需要了解更详细的内容，请访问官方文档 小结完成上述配置后，HBase可以正常访问了，基础的存储与计算都配置完成。下篇文件我们开始《Jupyter 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 部署]]></title>
    <url>%2Fai%2Fhadoop-spk%2F</url>
    <content type="text"><![CDATA[Spark是内存式计算引擎，为了让我们的计算速度更快，计算更多复杂的模型。这篇文章我们部署它，之后我们编写的代码都将跑在Spark中。 Quick Start下载安装登陆官方网站，下载版本为2.2.1，下载完成后解压（文件目录还是统一放在/opt路径下）并进入该文件夹，运行如下命令： 注：从2.0版本开始，缺省支持Scala2.11版本，如果你习惯使用其他版本的Scala，请查看官网 1cp conf/spark-env.sh.template conf/spark-env.sh 环境变量配置在conf/spark-env.sh文件中添加如下配置项： Java环境变量1export JAVA_HOME=$JAVA_HOME 如果未设置Java环境变量，请自行添加就好。 Client模式运行时，所需的参数环境变量123456789101112131415# yarn集群中，最多能够同时启动的Executors的实例个数。# yarn中实际能够启动的最大Executors的数量会小于等于该值。如果不能确定最大能够启动的Executors数量，建议将该值先设置的足够大。export SPARK_ EXECUTOR_INSTANCES=9# 该参数为设置每个Executor能够使用的CPU核数# yarn集群能够最多并行的task数量为SPARK_EXECUTOR_INSTANCES ＊ SPARK_EXECUTOR_CORESexport SPARK_EXECUTOR_CORES=2# 该参数设置的是每个Executor分配的内存的数量。# 需要注意的是，该内存数量是SPARK_EXECUTOR_CORES中设置的内核数共用的内存数量。export SPARK_EXECUTOR_MEMORY=8G#该参数设置的是DRIVER分配的内存的大小。#也就是执行start-thriftserver.sh机器上分配给thriftserver的内存大小。export SPARK_DRIVER_MEMORY=8G ZooKeeper环境变量1export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node0的IP:2181,node2的IP:2181,node3IP:2181" 其他环境变量12345export HADOOP_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoopexport YARN_CONF_DIR=/opt/hadoop-2.7.4/etc/hadoopexport SPARK_HOME=/opt/spark-2.2.0-bin-hadoop2.7export SPARK_JAR=$SPARK_HOME/jars/*.jarexport PATH=$SPARK_HOME/bin:$PATH 从属节点配置复制文件1cp conf/slaves.template conf/slaves 配置地址打开slaves文件，填写如下内容： 123node1的IPnode2的IPnode3的IP 测试我们用spark文件夹中自带的求Pi例子做测试 单机模式（本地模式）1bin/spark-submit --master yarn-client --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.2.0.jar 集群模式1bin/spark-submit --master yarn-cluster --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.2.0.jar 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本2.2.1 小结完成这篇的配置后，我们可以做大部分数据挖掘工作了，但数据只能使用csv文件，为了让我们可以使用数据库，快速查找，快速计算，快速存取。下篇文件我们开始《HBase 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 基础教程[系列]]]></title>
    <url>%2Fai%2Fhadoop-tutorial%2F</url>
    <content type="text"><![CDATA[前言这一系列的文章主要介绍，Hadoop基础，说明各个工具的作用及用途，相互之间的关系。 目录 HDFS MapReduce Zookeeper 关系架构 Spark HBase Jupiter]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 部署]]></title>
    <url>%2Fai%2Fhadoop-dfs%2F</url>
    <content type="text"><![CDATA[这篇文章我们将按照规划方案配置HDFS，从4台中任一选择一台进行配置，本文选择node0。 Quick Start下载及Java配置登陆官方网站，下载hadoop.tar.gz文件，本文所使用的版本为2.7.4，下载完成后解压并进入该文件夹，修改etc/hadoop/hadoop-env.sh文件 1JAVA_HOME=/opt/jdk1.8.0_65 注：因为Hadoop相关的工具比较多，可以把所有工具统一放在相同文件路径下，即使在不同服务器中也可以方便查找，本文将统一放在/opt路径下 HDFS配置配置etc/hadoop/hdfs-site.xml，将下面的XML标签项添加在&lt;configuration&gt;标签内。 注：配置项中需要填写IP地址的地方，强烈推荐填写IP地址，不要使用主机名。（在访问页面时，方便大家在不做hosts文件修改时，正常跳转） 服务名1234&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt; NameNode服务的名字1234&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt; NameNode的RPC协议与端口配置该项后，可以通过程序调用8020接口，RPC协议主要用于系统内部通信以及用户编程访问。 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node0的IP:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node1的IP:8020&lt;/value&gt;&lt;/property&gt; NameNode的HTTP协议与端口配置该项后，可以通过浏览器访问50070接口 12345678&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node0的IP:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;node1的IP:50070&lt;/value&gt;&lt;/property&gt; 固定配置，客户端通过该类找到active的NameNode1234&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; SSH安全12345678&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt; JournalNode的地址与端口1234&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node1的IP:8485;node2的IP:8485;node3的IP:8485/mycluster&lt;/value&gt;&lt;/property&gt; JournalNode的工作目录1234&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;此处填写你希望保存的路径即可，本文放在 /opt/hadoop-2.7.4/journalnode.edits&lt;/value&gt;&lt;/property&gt; ZKFC自动切换1234&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 打开权限控制1234&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; slaves文件配置方式配置datanode时，如果不是使用了主机名加DNS解析或者hosts文件解析的方式，而是直接使用ip地址去配置slaves文件 1234&lt;property&gt; &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; CORE配置配置etc/hadoop/core-site.xml，将下面的XML标签项添加在&lt;configuration&gt;标签内。 NameNode入口1234&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt; ZooKeeper地址与端口1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node0:2181,node2:2181,node3:2181&lt;/value&gt;&lt;/property&gt; NameNode原数据存储目录1234&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;可自定义设置，本文存储路径 /opt/hadoop-2.7.4/tmp&lt;/value&gt;&lt;/property&gt; 其他配置项指定DataNode地址在etc/hadoop文件下，创建slaves文件，内容如下： 注：此处可以填写IP地址，也可填写主机名，推荐IP地址，保持配置一致性 123node1的IPnode2的IPnode3的IP 分发工具因为Hadoop会使用到所有的服务器，所以你必须将它分发到你所有的机器节点上，本教程一共4台服务器，所以将Hadoop文件夹分发到其他3台。 123scp -r /opt/hadoop-2.7.4 root@node1:/optscp -r /opt/hadoop-2.7.4 root@node2:/optscp -r /opt/hadoop-2.7.4 root@node3:/opt 启动JournalNode按照规划我们并没有把JournalNode服务部署在所有服务器节点上，所以，这里需要分别启动node1，node2，node3上的JournalNode进程。 1sbin/hadoop-daemon.sh start journalnode 使用jps命令查看是否启动成功，显示PID JournalNode则为成功。 格式化NameNode就像我们新装windows操作系统一样，需要磁盘格式化，从而建立该系统的元数据。 在node0与node1之间，选择任一选择（这里选择node0），运行如下命令： 1bin/hdfs namenode -format 格式化成功后会在tmp/dfs/name/current/目录下生成fsimage元数据 启动NameNode服务注：启动node0节点上的NameNode服务，目的是拷贝刚刚格式化好的元数据到node1中。 1sbin/hadoop-daemon.sh start namenode 注：如果启动失败，可以删除之前生成的元数据，重新格式化。 拷贝元数据将node0中的NameNode服务正常启动后，就可以拷贝元数据到node1中了。 注：下面这条命令必须在node1中执行。 1bin/hdfs namenode -bootstrapStandby 查看拷贝是否成功。可在node1中tmp/dfs/name/current/目录下查看是否生成fsimage元数据。 注：如果格式化NameNode与拷贝元数据这几步中依然出现莫名的错误，可以删除2个节点上的元数据，重新选择另一台机器（这里选择node1）从格式化NameNode步骤开始，再做一遍。（笔者之前就遇到过此类莫名其妙的问题- -!） 格式化DFSZKFailoverController 进行此步之前，需要关闭所有dfs服务： 1sbin/stop-dfs.sh 格式化ZKFC: 1bin/hdfs zkfc -formatZK 启动HDFS服务完成以上步骤后，就可以启动HDFS服务了，在4台节点中，任一选择一台键入命令，都可以启动所有节点的服务。这里推荐使用node0。 1sbin/start-dfs.sh 访问与测试正常启动HDFS服务后，再node1中使用jps命令看到NameNode，JournalNode，DFSZKFailoverController，DataNode服务。 HTTP访问HDFS服务通过IP地址:50070接口，在浏览器中正常访问到HDFS。 注：node0与node1中，一台是active状态，一台是standby状态，由ZooKeeper服务投票决定。 手动切换如果启动HDFS时，两个NameNode都处于standby状态，我们也可以手动指定一台节点为激活状态。本文指定nn2（这里使用配置项中的NameNode服务名） 1bin/hdfs haadmin -transitionToActive --forcemanual nn2 active状态：transitionToActive，standby状态：transitionToStandby HTTP访问计算引擎通过IP地址:8088接口，在浏览器中正常访问到计算引擎。 测试 创建test目录 1bin/hdfs dfs -mkdir -p /test 上传hello.txt文件到test目录 1bin/hdfs dfs -put hello.txt /test/ 此时可以在web页面中，查看刚刚创建的文件夹在Utilities -&gt; Browse the file system下查看 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结完成上述配置后，HDFS可以正常访问了，HDFS很多操作能够正常使用，MapReduce是必不可少的。随着HDFS的配置完成，说明MapReduce也配置完成。下篇文件我们开始《YARN 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN 部署]]></title>
    <url>%2Fai%2Fhadoop-yrn%2F</url>
    <content type="text"><![CDATA[上一篇文章我们已经完成了HDFS系统的部署，接下来我们开始YARN的配置，它是资源调度很重要的部分。依然选择在node0节点上进行配置。 Quick StartMapReduce 资源调度配置Hadoop中计算引擎的运行方式有很多，在企业级应用中我们选择yarn作为资源调度的方式。 配置MapReduce的资源调度方式复制etc/hadoop/mapred-site.xml.template为mapred-site.xml，并添加如下配置项： 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; YARN 配置在etc/hadoop/yarn-site.xml文件中，添加如下配置项： 资源申请的主机1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node0的IP&lt;/value&gt;&lt;/property&gt; NodeManager 附属服务1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 服务类（固定配置）1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt; 注：下面打问号的6个配置项，需要计算，才能得出（依据自己机器的节点数，每个节点的具体硬件配置，以及各自的任务需要）。这也是yarn配置的重点。如果想了解YARN在不同数量及配置的服务器中该如何计算，请点击此处，进行查看。 内存总量1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 最小可申请内存量1234&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; yarn启动时分配给AppMaster的默认内存大小1234&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; JVM参数1234&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 可供调用的CPU线程数这个配置项指你分配多少个CPU线程供yarn调度，可以全部，也可以分配一部分，主要看这个节点的想如何使用。 1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 单个任务可申请的最多CPU线程数1234&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;?&lt;/value&gt;&lt;/property&gt; 计算好数值，将问号处填写好后，yarn的配置就完成了。 启动服务如果你之前已经启动了HDFS服务，这里只需系统yarn服务就可以了。 1sbin/start-yarn.sh 如果未启动，可以使用如下命令，启动Hadoop全部服务。 1sbin/start-all.sh 测试可以安装如下命令格式，提交编写好的jar包，及数据提交给MapReduce进行计算。 bin/hadoop jar [x.jar] [hdfs://数据所在目录] [hdfs://结果导出目录] 结果返回到指定hdfs目录。结果集可以使用hdfs命令行查看，也可取回本地查看。 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结配置好YARN之后，我们就可以编写代码利用MapReduce完成基本的数据挖掘工作了，但MapReduce作为离线计算框架，在速度方面并不能让我们满意，我们需要更快速更灵活的计算框架。下篇文件我们开始《Spark 部署》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 部署]]></title>
    <url>%2Fai%2Fhadoop-zkp%2F</url>
    <content type="text"><![CDATA[ZooKeeper是分布式应用程序协调服务，在分布式系统中必不可少。它是为分布式应用提供一致性服务的软件，所以我们首先来安装配置它。 安装前，我们需要先准备好安装包，点击官方下载地址，本文所使用的版本是3.4.10 Quick Start创建目录 在解压后的文件夹中创建一个名为tmp文件夹，作为其工作目录。 再创建一个名为zk_data文件夹，作为其数据存储目录。 配置 拷贝conf/zoo_sample.cfg文件，并重命名zoo.cfg （这里必须命名为zoo.cfg） 修改zoo.cfg文件，内容如下： 12345678tickTime=2000dataDir=/opt/zookeeper-3.4.10/zk_dataclientPort=2181initLimit=5syncLimit=2server.1=node0的IP:2888:3888server.2=node2的IP:2888:3888server.3=node3的IP:2888:3888 将此配置分别配置到 node0， node2， node3中 按照规划ZooKeeper会被部署在node0，node2，node3上，所以需要在这三台服务器中都拷贝一份。 12scp -r /opt/zookeeper-3.4.10 root@node2:/optscp -r /opt/zookeeper-3.4.10 root@node3:/opt 分别在这三个节点的dataDir指向的目录下创建myid文件，值分别为1，2，3 在node0，node2，node3中依次启动服务，命令如下： 1bin/zkServer.sh start 查看服务是否已经启动 1zkserver.sh status 如果遇到Error contacting service. It is probably not running 此错误，请查看对应版本的官方文档重新配置zoo.cfg。 如果是java.net.BindException: Address already in use 通过netstat -nltp | grep 2181检查是否该端口已被占用。 如果，遇到防火墙原因，请继续往下看。 关闭防火墙​关闭所有服务器的防火墙（重点） firewall 查看防火墙状态 1firewall-cmd --state 关闭防火墙 1systemctl stop firewalld.service 禁止开机启动 1systemctl disable firewalld.service 关闭iptables 1service iptables stop 官方文档如果需要了解更详细的内容，请访问官方文档，文档版本3.4.10 小结完成上述配置后，ZooKeeper应该可以正常启动了，下篇文件我们开始《部署 HDFS》 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式平台前期规划]]></title>
    <url>%2Fai%2Fhadoop-planning%2F</url>
    <content type="text"><![CDATA[完成上一篇文章《服务器批量安装》的内容后，我们已经拥有了4台Linux服务器，且相互之间网络可以互通，并且正常运行SHH服务。硬件环境已经准备完成，这篇文章我们将开始讲述Hadoop前期规划的准备工作。Hadoop是一系列工具的集合，如何合理的规划这些工具以及分配服务器资源，是一个非常重要的工作。 Quick Start主机名配置我将分别修改主机名为node0，node1，node2，node3。方便教程的讲述，也方便ssh中的操作。选择其中一台服务器，root用户登陆。 查看主机名1hostname 修改主机名 方法一：修改network文件，将HOSTNAME后面的值改为node0，重启后生效。 1vim /etc/sysconfig/network 方法二：修改当前的主机名，立即生效。 1hostname node0 配置hosts文件修改/etc/hosts文件，ip0为你自己机器的ip地址 1234ip0 node0ip1 node1ip2 node2ip3 node3 分发hosts文件将hosts文件分发到其他3台机器中，以保证所有服务器识别主机名。 123scp /etc/hosts root@node1:/etc/scp /etc/hosts root@node2:/etc/scp /etc/hosts root@node3:/etc/ 批量管理工具推荐如果你想更快，更省力的完成批量操作，有兴趣的童鞋可以安装Linux集群批量管理工具parallel-ssh(PSSH)，该工具需要Python环境，安装及操作点击此处链接，该教程中还是采用普通命令进行讲述。 配置免密码登录之后的服务器命令都需要免密码才能正常操作，这是一个必须而重要的步骤。我以node0批量操作其他服务器为例。此步骤需要在所有服务器中完成一边，以方便任意两台机器可以互相登陆。 创建本机的公钥与私钥12cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 分发公钥到其他3台服务器我以node1为例 1scp ~/.ssh/id_rsa.pub root@node1:~/ 公钥追加进入node1的root账户的home目录下，运行如下命令。完成后，就可以从node0免密码登录到node1了。 1cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 安装Java环境hadoop整套工具都以Java环境为基础，所以4台机器都需要安装。我们以node0为例。 查看是否安装1syum list installed | grep java 查看yum库中的Java安装包1yum -y list java* 安装Java我们以版本1.8.0为例 1yum -y install java-1.8.0-openjdk* 配置环境变量 将jdk文件夹移动到opt文件夹下 在/etc/profile文件中追加如下内容： 12export JAVA_HOME=/opt/jdk1.8.0_65export PATH=$JAVA_HOME/bin:$PATH JSP进程集工具集介绍在此篇基础工具集的规划中，我们主要安装Hadoop, ZooKeeper, HBase, Spark, Jupiter, Thrift。之后的教程中还会讲到Hive，MySQL等。每种工具都对应着一些Java进程，我们将规划这些进程分别部署到哪个服务器上。（话说，分布式应用，总不能把所有的进程都安装在一台服务器中吧。。。 - -!） 注：如果你对上述工具还不熟悉，请跳转到《Hadoop 基础教程》 JSP进程JSP是Java Virtual Machine Process Status Tool的缩写，在JVM中所有具有访问权限的Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与Linux上的ps命令类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 工具 JPS进程 ZooKeeper QuorumPeerMain HDFS NameNode, DataNode, JournalNode, ZKFailoverController MapReduce, Spark ResourceManager, NodeManager HBase HMaster, HRegionServer Thrift ThriftServer 系统进程 工具 系统进程 Jupyter jupyter-notebook 进程规划规划图根据上一篇的教程，我们准备了4台服务器，我们将把上面介绍的进程部署在这4台服务器中，方案如下： 图中勾选的位置对应着进程将部署在哪台服务器。 缩写对照表 缩写 进程全称 NN NameNode DN DataNode ZK QuorumPeerMain ZKFC ZKFailoverController JN JournalNode RM ResourceManager NM NodeManager HM HMaster HR HRegionServer TS ThriftServer 小结此篇主要介绍平台安装前的准备工作，以及要部署的工具集与JSP进程的规划方案，当然这个规划方案是以4台服务器为基础，如果你的服务器数量超过4台（无论怎样要大于等于3台，原因可以在《Hadoop 基础教程》中了解，此处不在赘述！）规划方案可以相应调整，下篇《ZooKeeper 部署》。 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KOA 开始]]></title>
    <url>%2Fkoa%2Fkoa-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>koa</category>
      </categories>
      <tags>
        <tag>NodeJS</tag>
        <tag>全栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器批量安装]]></title>
    <url>%2Fai%2Fhadoop-servers%2F</url>
    <content type="text"><![CDATA[在安装Hadoop分布式系统之前，我们需要准备好服务器资源，如果采用云服务器，可以跳过此篇文章。批量无人值守安装操作系统，此次示例系统为CentOS7.x且推荐安装X window用户界面，后面会用到，服务器数量为4台，当然你可以使用大于等于3台以上数量的机器。 在4台机器中，随意选择1台安装服务。注：推荐直接使用实体机进行安装，或者非VMware的虚拟机，否则无人值守批量安装系统时可能会出错。 Quick Start安装FTP服务安装vsftp服务：1yum -y install vsftpd 启动vsftp服务：1systemctl start vsftpd.service 将准备好的系统iso文件加载到光驱如果使用虚拟机，则加载iso文件，如果是实体机可使用光盘或U盘加载系统文件。 将光驱文件挂在到ftp目录下：1mount /dev/cdrom /var/ftp/pub 测试FTP服务是否可以匿名登录，命令如下： 如果系统提示lftp服务未安装，安装lftp 1yum -y install lftp 进入lftp模式后会看到pub文件夹，如果没有，请关闭防火墙和selinux 关闭防火墙：systemctl stop firewalld.service查看selinux：getenforce暂时关闭selinux：setenforce 0永久关闭selinux：修改/etc/selinux/config文件SELINUX=enforcing改为SELINUX=disabled，重启即可 进入pub文件夹，如果有文件，测试正常 安装PXE并生成pxelinux.0启动文件安装syslinux服务：1yum install -y syslinux 查询文件所在目录：1rpm -ql syslinux | grep "pxelinux.0" 安装TFTP服务安装tftp服务：1yum -y install tftp-server 修改配置文件：打开/etc/xinetd.d/tftp文件，修改disable=no 启动tftp服务：1systemctl start xinetd.service 查看server_args的值找到tftpboot文件夹路径（通常为/var/lib/tftpboot）1systemctl start xinetd.service 拷贝文件在tftpboot文件夹下，新建文件夹pxelinux.cfg，并执行如下命令：1234cp /usr/share/syslinux/pxelinux.0 .cp /var/ftp/pub/isolinux/isolinux.cfg ./pxelinux.cfg/defaultcp /var/ftp/pbu/isolinux/vmlinuz .cp /var/ftp/pbu/isolinux/initrd.img . 设置权限设置./pxelinux.cfg/default的权限为644： 1chmod 644 default 安装DHCP服务安装dhcp服务1yum -y install dhcp 配置/etc/dhcp/dhcpd.conf：123456789101112131415ddns-update-style interim;allow booting;allow booting;next-server 192.168.0.1;filename "pxelinux.0";default-lease-time 1800;max-lease-time 7200;ping-check true;option domain-name-servers 192.168.0.1;subnet 192.168.0.0 netmask 255.255.255.0&#123; range 192.168.0.100 192.168.0.220; option routers 192.168.0.1; option broadcast-address 192.168.0.255;&#125; 启动HDCP：1systemctl start dhcpd.service 安装Kickstart工具安装:1yum -y install system-config-kickstart 运行Kickstart并配置选项启动Kickstart Configurator界面（该软件需要系统安装X window）1system-config-kickstart 配置选项页Basic Configuration： Time Zone设置为Asia/Shanghai 勾选 Use UTC clock 设置Root Password与Confirm Password 勾选Reboot system after installation 配置选项页Installation Method： 在Installation source选框中 点选 FTP 填写FTP Server： 192.168.0.1 填写FTP Directory： pub 配置选项页Boot Loader Options： 点选 Install new boot loader 配置选项页Partition Information 勾选 Clear Master Boot Record 勾选 Remove all existing partitions 勾选 Initialize the disk label 点击Add 自定义分区 创建分区 新增 /boot分区 文件系统类型xfs或者ext4 Fixed size: 200MB 新增 /swap分区(在File System Type中选择) Fixed size: 2048MB 新增 / 分区 点选Fill all unused space on disk 创建完成后点击OK 配置选项页Network COnfiguration：点击Add Network Device, 下拉菜单中选择DHCP, 如果Network Device为空，请填写自己的网卡设备 配置选项页Fireswall Configuration： SELinux下拉选项：Disabled Security level下拉选项：Disable firewall 保存选项到文件完成配置并保存到/var/ftp/ks/ks.cfg 修改启动引导文件文件路径为/var/lib/tftpboot/pxelinux.cfg/default 1234timeout 60 //暂定时间label ks //选项kernel vmlinuzappend ks=ftp://192.168.0.1/ks/ks.cfg initrd=initrd.img 类似如下图： 小结到此，无人值守服务已全部配置完成，分别开启其他3台机器后，可自动进入系统安装。下篇《分布式平台前期规划》。 本系列文章《目录》]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React Native 开始]]></title>
    <url>%2Freact%2Frn-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>react</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 系统搭建[系列]]]></title>
    <url>%2Fai%2Fhadoop-start%2F</url>
    <content type="text"><![CDATA[前言这一系列的文章主要介绍，Hadoop分布式系统如何从硬件到软件搭建完成，相关插件的开发及使用的教程 目录 服务器批量安装 分布式平台前期规划 ZooKeeper 部署 HDFS 部署 YARN 部署 Spark 部署 HBase 部署 Jupyter 部署 Thrift 部署 Hadoop 附录]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用监控]]></title>
    <url>%2Fother%2Flinux-monitor%2F</url>
    <content type="text"><![CDATA[知道程序在服务器中运行时，对硬件的使用率有多少，尤其是长时间，大规模的运算任务，还是一件很重要的事。这篇文章简单介绍几个性能分析工具。 Quick StartCPU及内存监控top命令是Linux下常用的性能分析工具，能够实时显示CPU的使用率及系统中各个进程的资源占用状况。以每秒更新的方式显示实时情况。 基础信息1top 基础操纵点击下面的健会显示相应信息，点击ESC返回主界面 h: 帮助1: 显示CPU详细信息，每行代表一个线程f: 查看列信息n: 按下n键后，再按相应的数字，表示现实进程前几行z: 颜色模式q: 退出。或Ctrl+C 显示与隐藏l: 第一行负载信息t: CPU线程信息m: 内存信息 显卡监控如下命令主要适用于NVIDIA产品 1nvidia-msi IO监控基础方法每隔1秒采样一次，以KB的方式显示。 1iostat -d -x -k 1 如果显示有限次数，则在命令最后增加一个次数，下面命令显示5次 1iostat -d -x -k 1 5 参数说明d: 采样x: 详细信息，使用率c: CPU状态]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 硬件及系统信息查询]]></title>
    <url>%2Fother%2Flinux-info%2F</url>
    <content type="text"><![CDATA[最近在做一些与服务器硬件相关的事情，总是需要查询服务器的硬件信息（Linux上查询这些信息的确没有Windows上简单直观，且很多命令对我来说并不常用，完全记不住啊… - -!） 这文章，帮助自己下次查询，有用的上的朋友也可以look look ：） Quick Start主板基础信息1dmidecode | more 内存次插槽数，及每个槽位内存条大小1dmidecode|grep -P -A5 "Memory\s+Device"|grep Size|grep -v Range 查看最大支持内存容量1dmidecode|grep -P 'Maximum\s+Capacity' CPU基础信息1cat /proc/cpuinfo 物理CPU个数1cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l 单个物理CPU的核数1cat /proc/cpuinfo| grep "cpu cores"| uniq 单个物理CPU的线程数（逻辑CPU）1cat /proc/cpuinfo| grep "processor"| wc -l 是否开启超线程 逻辑CPU &gt; 物理CPU x CPU核数 （已开启超线程） 逻辑CPU = 物理CPU x CPU核数 （未开启超线程或不支持超线程） 内存基础信息1cat /proc/meminfo 内存使用及缓存信息1free 硬盘基础信息1fdisk -l 磁盘分区1df 外设键盘鼠标1cat /proc/bus/input/devices PCI基础信息1lspci 查看系统版本查看内核版本1cat /proc/version 查看发行版本1cat /etc/issue 查看issue有些系统返回为空，如果是redhat系列的版本可用如下命令查看： 1cat /etc/redhat-release]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NBatis 开始]]></title>
    <url>%2Fnbatis%2Fnbatis-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>nbatis</category>
      </categories>
      <tags>
        <tag>NodeJS</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReactJS 开始]]></title>
    <url>%2Freact%2Freactjs-start%2F</url>
    <content type="text"><![CDATA[更新中…]]></content>
      <categories>
        <category>react</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>SPA</tag>
      </tags>
  </entry>
</search>
