---
title: 深入学习硬件那些事...
categories:
  - ai
tags:
  - 深度学习
date: 2018-03-20 10:18:18
---


提到深度学习，想必大家都知道这是件计算密集型的事，第一个蒙B的困扰是：“攒一个适合做这件事的系统到底需不需要买一个多核高速的CPU...- -？”。“要想性能猛，就得花钱糟”，带着这样的想法，我们在搭建深度学习系统时，做的最糟糕的事就是在硬件上花了很多冤枉钱。这篇文章，我将告诉大家如果一步步的攒出一个性能高端大气，价格童叟无欺的牛X系统。

那时，开始并行深度学习方面的工作，我需要搭建一个GPU集群。因为经验不足，所以我想：“得仔细挑选才行”。经管自己做了不少的研究，推理，但在选择硬件这件事上还是走了不少弯路。真是实践出真知，随着不断对集群的测试，才慢慢也抹出点门道。在这儿，我想把自己学到的东西分享出来，避免大家踩同样的坑。

GPU

这篇文章主要关于如何用GPU进行深度学习。如果此时，你正想搭建或者升级你的系统去做机器学习方面的事，那么我想告诉你：“GPU真是太tm重要了”。当然，GPU也只是深度学习应用程序的核心（它尤其在整体系统处理速度提高方面帮助，不容忽视）

[在我之前的文章](https://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/)中就详细讨论过GPU的选择，因为这是深度学习系统搭建中的致命一环。通常来说，如果你的预算比较羞涩的话，个人比较推荐去网上淘个GTX 680。当然如果你是土豪（别忘了留下联系方式，大家可以做朋友∩_∩），在小规模的卷积神经网络方面，我推荐GTX TITAN X。稍大一些规划的卷积神经网络，GTX 980（这也许是当时最好的GPU） 效果会更好。还是那句话，如果你想价格便宜点还是去网上淘吧！我之前一直在用GTX 580，但由于cuDNN库的更新（新版本在卷积速度方面提高的一大截），GTX 580已无法支持。如果你不拿它做卷积神经网络方面的事。个人觉得580还是一个不错的选择。

下面这张图你能看出那块牛X，哪块令你蛋碎吗？
![]()

CPU

为了能够对CPU做出明智的选择，我们首先得了解CPU与深度学习的关系，以及它到底在深度学习方面起什么作用。当你在GPU上运行你的深度学习算法时，这时的CPU到底在忙些什么？下面我将为你列这货在干的事：

编写和读取代码中的变量
执行指令（比如调个函数啥的）
在GPU上初始化函数调用
样本数据mini-batch
把准备好的子集循环传给GPU

注：如果你还不了解什么是mini-batch，[请点击此链接](https://testerhome.com/topics/10877)。

需要的CPU核心数

当我用三个不同的库训练同样的神经网络时，CPU线程占用率都是100%（有时，另一个线程会在0到100%之前波动一会）。从这点可以看出，大部分到深度学习使用的库（实际上大部分软件程序）只使用一个线程运行。这意味着，一般情况下，多核CPU基本没啥用。但是如果你同时运行多个GPU或者使用像MPI这样的并行框架，多个程序的同时运行，多线程就显得很重要了。通常情况下，一个GPU使用一个CPU线程，但如果一个GPU可以使用两个线程，那会大大提高深度学习库的性能。由于这些库在一个核上运行，因为有时会异步调用函数，所以会使用第二个CPU线程。敲黑板了，知识点啊：现在CPU的每个核都会有多个CPU线程（在Intel的CPUs中这点尤为突出）因此单核的CPU通常已经足够。

CPU and PCI-Express

注：一些Haswell架构的GPU并不支持完整的40路PCI-E通道，如果你想搭建多GPU系统，确保你的CPU和主板支持PCI-E 3.0

CPU 缓存大小

下面我会从CPU到GPU运行的过程，做一个大致的分析，告诉你关于CPU缓存的故事。对整个过程有了透彻的理解，我们才能从中找到可能存在瓶颈的地方。

当我们购买CPU时，CPU缓存常常被忽略（有木有看到缓存们悲愤的眼神'_'），但其实，他们在整体性能难题中扮演着非常重要的角色。CPU缓存时一组数量非常少的芯片内存，他们被集成在CPU中，用于执行高速的运算及指令操作。CPU中的缓存通常存在一个层级结构，从小而快（L1, L2）到大而慢（L3, L4）。作为一个程序猿，你应该知道它就像个哈希表，每条记录就是一个键值对。它可以对特殊的健做非常快速的查找：如果找到的健，就可以对其值进行快速的读写操作；如果未找到（缓存缺失），CPU将会去内存中查找，然后从那把值读取出来（这是一个非常缓慢的过程，如同你遇到了那个叫闪电的家伙）。上述过程我们可以看到高效的CPU缓存过程体系结构通常对CPU性能至关重要。

CPU是如何确定他的缓存过程是一个非常复杂的议题。不过大致可以这样理解，重复使用频率高的变量，指令以及内存地址将会被缓存，其他的则不会。

现在我们分析在深度学习中缓存的使用情况。数据在发送到GPU处理前，相同内存空间的数据以mini-batch块的形式被重复读取，那么这部分样本数据就可以存储在缓存中，当然这也取决于mini-batch的大小。我们取每个mini-batch块中128个样本，就好像我们有0.4MB的[MNIST](https://www.cnblogs.com/lizheng114/p/7439556.html)和1.5MB的[CIFAR](https://blog.csdn.net/zeuseign/article/details/72773342)，这适合大多数CPU缓存。但是，在[ImageNet](https://baike.baidu.com/item/ImageNet/17752829?fr=aladdin)中，每个mini-batch却高达85MB的大小（4 * 128 * 244^2 * 3* 1024^-2），这个数量远远大于CPU最大量级的缓存（L3也只有那么有限的2，30MB）。

通常情况下，样本数据都非常的大而无法放入缓存。每个新的mini-batch块都需要从内存中重新读区。所以无论怎样，都会不断的访问内存。

也许你会想到，可以把样本数据都精确内存地址放入缓存，这样CPU在执行时就可以高速查找了。可真如我们所愿吗？如果这样，你必须把整个样本集都放入内存，否则查找时内存地址就会发生变化，这样依然无法依靠缓存提速，比如我们使用[页锁内存](https://blog.csdn.net/ziv555/article/details/52116877)时。

当然，代码中的变量和函数调用还是得益于高速缓存，但这些通常数量很少，很容易把它们放进CPU的L1缓存内。

从上述内容中我们可以得出结论，在深度学习方面，CPU的缓存大小似乎没那么重要。下一节，我们会进一步分析验证这个结论。

CPU时钟频率

当人们形容CPU的快慢时，通常首先会想到时钟频率。4GHz比3.5GHz好，是这样吗？一般来说两个相同架构的处理器（比如都是 Ivy Bridge）可以这样看待，但是无法比较出两个处理器的优劣。它并不是一个好的衡量性能的度量衡。

在深度学习领域，CPU的计算量其实非常的小（增加几个变量，计算一些布尔表达式，在GPU的计算过程中调用几个函数，当然这些事情都取决于CPU的核心时钟频率）。这么推理似乎也挺合理，但当我们运行代码时，却发现CPU的使用率高达100%，这么点操作使用率却如此之高，究竟问题出在哪？于是，我做了一些CPU降频的实验，发现问题所在。

在CPU的使用率高达100%时，如果与其主频无关的话，应该与神马有关呢？答案也许是CPU缓存缺失：CPU一只非常忙碌的在访问内存，但同时CPU不得不等待内存的低频率，结果导致奇怪的忙等待状态。如果这个推论是正确的，那么CPU降频也不会导致性能的急剧下降，正如上图所示。

当然，CPU还会执行一些其他的操作，例如：拷贝数据到mini-batches，把准备好的数据拷贝到GPU，但这些操作的速度依赖于内存频率，并不取决于CPU主频。那么现在我嘛来看看内存。

内存频率

CPU与内存以及与其他硬件的相互作用是一个强大复杂的过程。本文将此简化，来进行研究CPU到内存再到显存到整个过程。以便帮助大家更透彻的理解。

CPU时钟与内存被纠缠在一起，CPU的主频决定了内存的最大频率，且这两者决定了你的CPU带宽，但是，通常来说内存自己决定了全部可用带宽，因为它的速度慢于CPU速率。你可以这样来计算带宽：bandwidth in GB/s = RAM clock in GHz * memory channels of CPU * 64 / 8

这里的64是指64位CPU架构。通过计算我的处理器与内存带宽位51.2GB/s

通常情况下，如果要拷贝大批量数据，那带宽与性能将息息相关。例如：CPU需要匹配内存的速度，从内存中获取大批量小块数据。这种情况，几乎适合所有的深度学习相关的程序。要么数据块大小可以放进缓存，那很容易从中获益，要么数据块太大而无法有效利用缓存。从上述例子中看到带宽比缓存似乎更加重要。

那么，这与深度学习程序有什么关系呢？我刚才说过带宽似乎是很重要的，但我们继续往下看时，发现情况似乎又发生了变化。内存带宽决定了一个mini-batch以多快的速度呗覆盖以及分配去初始化GPU传输，但继续下一步时，我们会发现CPU到内存到GPU再回到内存这个过程存在瓶颈（直接内存访问）。之前提到过，我的机器带宽时51.2GB/s，但DMA带宽只有12GB/s。